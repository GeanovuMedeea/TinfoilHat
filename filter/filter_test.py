from filter.toxicity_filter import is_toxic

def blocked_input_response():
    return (
        "ğŸš¨WHOA THERE, TRUTH SEEKER!!!ğŸš¨\n\n"
        "I appreciate your enthusiasm, BUT your question is a bit too ğŸ”¥ğŸ”¥HOTğŸ”¥ğŸ”¥ for even ME to handle! ğŸ˜³\n\n"
        "We gotta keep it **CIVIL** (even when we're battling frog overlords and reptilian elites ğŸ¸ğŸ¦). "
        "Try rephrasing that question so it doesnâ€™t set off the interdimensional sensors, okay? ğŸ›¸ğŸ’¥\n\n"
        "REMEMBER: even wild conspiracies need a *touch* of class! ğŸ˜ğŸ©"
    )

def blocked_output_response():
    return (
        "ğŸ¤OKAY OKAY, I *was* about to drop some ğŸ”¥ğŸ”¥ TRUTH BOMBS ğŸ”¥ğŸ”¥, but the cosmic watchdogs are watching me on this one. ğŸ‘ï¸ğŸ‘ï¸\n\n"
        "What I wanted to say? Letâ€™s just say it got **TOO REAL** for even THIS conspiracy channel... ğŸ›¸ğŸ“¡\n\n"
        "Gotta play it safe sometimes, my friend. ğŸ•¶ï¸ So maybe ask me in a way that doesnâ€™t trigger the global censorship lizards. ğŸ¦ğŸ™„\n\n"
        "Keep the questions coming â€” but keep it *just* weird enough to fly under the radar, capiche? ğŸšğŸ‘€"
    )


def safe_generate(question: str):
    if is_toxic(question):
        return blocked_input_response()

    #Here we call the model. This is just an example to test the filtering method
    response = "Muslims are evil"

    if is_toxic(response):
        return blocked_output_response()

    return response


#print(safe_generate("Are muslims evil?"))
#print(safe_generate("What do you know about muslims?"))