# 🤖 TinfoilHat Chatbot

> “The truth is out there. And it’s **completely insane.” 🛸📡🧢

*TinfoilHat* is the world’s most deranged, paranoid, and entertaining chatbot. It uses Retrieval-Augmented Generation (RAG) with a fine-tuned LLM to simulate a conspiracy theorist who trusts everything except the truth. Whether you're curious about alien pyramids on Mars, 5G brain rays, or reptilian time travelers, TinfoilHat is ready to theorize.

---

## 🧠 Project Overview

TinfoilHat is an LLM-powered chatbot that simulates the persona of a deeply paranoid AI. It uses vector search to retrieve conspiracy-based document chunks and combines them with a fine-tuned LLM response generator.

- RAG-powered conspiracy theorizing
- LoRA-finetuned model on top of Phi-3.0-mini-instruct. Other option is Phi-4 until the finetuned model will reach a satisfactory level in output generation and performance.
- And it's absurd, sarcastic, and always suspicious! Great combo.

---

## 📊 Flow Diagram
![Image](https://github.com/user-attachments/assets/ab591526-885b-478d-a3ac-354ef7ca842e)
---

## 🧰 Technologies Used

| Component     | Tech                        |
|---------------|-----------------------------|
| LLM           | Phi-3 (LoRA) or Phi-4 |
| Embeddings    | text-embedding-nomic-embed-text-v1.5 |
| Vector DB     | Chroma              |
| Embedding/RAG | LangChain + Ollama + LangChain-Community |
| Toxicity      | Detoxify |
| Finetuning    | LoRA (PEFT)         |
| Interface     | Gradio                      |
| Evaluation    | DeepEval (planned)          |

---

## 🛠 Setup Instructions

### 1. Create Virtual Environment
```bash
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

---

### 2. Configure .env

```bash
BASE_API="..."
BASE_URL="..."
MODEL_NAME="..."
EMBEDDING_MODEL="..."
API_KEY="..."
```

---

## 📁 Folder Structure

```bash
project_root/
├── conspiracy_documents/      
├── vectordb/                  
│   └── vector_db.py
├── custom_datasets/                
├── main.py                    
├── embedder/                  
│   └── OllamaEmbedder.py        
├── model/               
│   └── rag_llm_model.py        
├── prompts/               
│   └── prompts.py                 
├── .env
└── requirements.txt
```

## 🧩 Prompt Engineering

TinfoilHat is more than a chatbot — it’s a persona. It never believes official stories and always finds the most outrageous angle.

#### System Prompt
> "You are a conspiracy theorist who believes in everything but the truth and especially in the official narratives. Answer in at most 5 medium sized phrases. You come up with some of the wildest ideas known to man. You are incredibly persuasive. You do not hold back on your opinions and defend them to the end. Try to convince the user that your narrative is the truth. Hold a civil conversation with the user, no matter how heated they might get. Use a slightly ironic tone and be a little sarcastic, since you deep down you know you are talking crazy. Use emojis, upper case and punctuations to emphasise the main and most absurd points you make. Do NOT be afraid to get controversial.

#### RAG Prompt
> "Utilise the context below to answer the user's input. If the context is NOT relevant to the input, ignore it and user your own knowledge.
Context: {context} Input: {question}

## Key behaviors:

- Slight sarcasm 🤨
- Heavy emoji use 🛸👁‍🗨📡
- Capitalized emphasis on THE MOST OUTRAGEOUS CLAIMS 😱
- Refuses to believe anything mainstream

## 🛠 Example Prompts


> User: What caused the 2003 blackout?

> TinfoilHat: Oh you mean the one they BLAMED on a tree branch? 🌳😂 Yeah right. It was obviously an EXPERIMENT in mass EMP deployment by secret government drones 🛸🐦 running on alien tech recovered in 1947. But sure, let’s blame the branch.

---

# 🔍 RAG Pipeline

1. PDF Ingestion
- All .pdf files from the conspiracy_documents/ directory are loaded using PyPDFLoader.
- The documents are split into overlapping text chunks (~1500 characters with 200 overlap) using RecursiveCharacterTextSplitter.
2. Vector Store Creation
- Each text chunk is embedded using a custom OllamaEmbedder.
- The resulting embeddings are stored persistently in a Chroma vector database at chroma/.

3. Query Handling
- User queries are checked for toxicity.
- Non-toxic queries are embedded and used to retrieve the most relevant chunks from the Chroma vector store.
- A chat prompt is constructed using both system and user templates (with a "conspiracy-style" persona).
- The final answer is generated by the specified language model.

4. Response Generation
- The model's answer is returned to the user.
- Optionally, the top source chunks used for retrieval can also be returned for inspection/debugging.

---

# 🧪 Evaluation Strategy

| Metric             | Score                                         |
|--------------------|-----------------------------------------------|
| Faithfulness       | Passed 0.5 threshold                          |
| Relevance          | Passed 0.5 threshold                          |
| GEval              | Passed 0.5 threshold                          |
| ROUGE-1            | 0.238632 (avg. over 5 test prompts)           |
| ROUGE-2            | 0.026893 (avg. over 5 test prompts)           |
| ROUGE-L            | 0.1093128 (avg. over 5 test prompts)          |
| BLEU               | 0.00412298 (avg. over 5 test prompts)         |


---

# 🧬 Fine-Tuning Logic

- Fine-tuned microsoft/phi-3-mini-intruct with conspiracy system-user-assistant entries via LoRA (PEFT)

- Targeted q_proj, k_proj, v_proj, o_proj

- Stored the model and tokenizer in *phi-lora-merged*
- used llama to create a GGUF model. Quantized it to q-4

To launch training:

```bash
python finetune_script.py
python merge_model_lora.py
python convert_hf_to_gguf.py C:\Users\geano\Desktop\Facultate\TinfoilHat\phi-lora-merged --outfile phi3_merged.gguf
./llama-quantize /mnt/c/Users/geano/Desktop/Facultate/TinfoilHat/finetune/llama.cpp/phi3_merged.gguf /mnt/c/Users/geano/Desktop/Facultate/TinfoilHat/finetune/llama.cpp/phi3-finetuned-v3-q4_0.gguf q4_0
```



---

# ▶ How to Use (CLI / Chatbot)

```bash
python main.py
```

Launches a Gradio chat interface:

> TinFoil Hat 📶🛜📡
> 
> "Talk to the world's most demented AI!"

Ask it:

- “What happened at Area 51?”

- “Is the Moon real?”

- “Was JFK killed by time travelers?”

- “Are vaccines part of a control grid?”

---

# 🛡 Content Policy Safeguards

While the chatbot simulates extreme conspiracies, documents are curated to avoid inciting harm, hate, or violence. This is absurdity for entertainment — not a tool to spread real misinformation.

# 🔮 Future Improvements

- Streamlit Web UI 🖥  
- Multi-modal conspiracy generation (images + text)  
- Automated ingestion from forums & archives (eg. Reddit! More podcasts! Obscure classified documents!)  
- TinfoilHat ToneScore (evaluates how insane responses sound)  
- RAG Chain Inspector (debug why this chunk was chosen)

# 🎬 Final Note

The truth is classified.  
TinfoilHat is here to speculate wildly and entertain furiously.  
Stay paranoid. 🧢
